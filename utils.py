#encoding:utf-8
import pandas as pd
import pickle
import numpy as np
from tqdm import tqdm
import os
import math
import random

#Function: Get the subscript corresponding to the value. The parameters are lists and characters
def item2id(data,w2i):
    #x is obtained directly in the dictionary, Return 'UNK' if not in the dictionary
    return [w2i[x] if x in w2i else w2i['UNK'] for x in data]
    
#----------------------------Function: splice files---------------------------------
def get_data_with_windows(name='train'):
    #Read the dict.pkl file generated by prepare_data.py and store the dictionary {type: subscript}
    with open(f'data/dict.pkl', 'rb') as f:
        map_dict = pickle.load(f)   #Load dictionary
        
    #Store all data
    results = []
    root = os.path.join('data/prepare/'+name)
    files = list(os.listdir(root))
    print(files)
    #['10.csv', '11.csv', '12.csv',.....]

    #Get all files
    for file in tqdm(files):
        all_data = []
        path = os.path.join(root, file)
        samples = pd.read_csv(path,sep=',')
        max_num = len(samples)
        #Get sep subscript
        sep_index = [-1]+samples[samples['word']=='sep'].index.tolist()+[max_num]
        #print(sep_index)
        #[-1, 83, 92, 117, 134, 158, 173, 200,......]

        #----------------------------------------------------------------------
        #     Get sentences and convert all sentences into corresponding ids
        #----------------------------------------------------------------------
        for i in range(len(sep_index)-1):
            start = sep_index[i] + 1     #0 (-1+1)
            end = sep_index[i+1]         #20
            data = []
            #Each feature is processed
            for feature in samples.columns:    #Visit each column
                #Obtain the second value of the subscript map_dict through the function item2id (list and dictionary)
                data.append(item2id(list(samples[feature])[start:end],map_dict[feature][1]))
            all_data.append(data)

        #----------------------------------------------------------------------
        #                          Data utils
        #----------------------------------------------------------------------
        #Combining two sentences before and after
        two = []
        for i in range(len(all_data)-1):
            first = all_data[i]
            second = all_data[i+1]
            two.append([first[k]+second[k] for k in range(len(first))])

        three = []
        for i in range(len(all_data)-2):
            first = all_data[i]
            second = all_data[i+1]
            third = all_data[i+2]
            three.append([first[k]+second[k]+third[k] for k in range(len(first))])
            
        #Return all results
        results.extend(all_data+two+three)
        
    #return results

    #Data is stored locally
    with open(f'data/'+name+'.pkl', 'wb') as f:
        pickle.dump(results, f)
        
#----------------------------Function: batch processing---------------------------------
class BatchManager(object):

    def __init__(self, batch_size, name='train'):
        #Call function combined with file
        #data = get_data_with_windows(name)
        
        #Read files
        with open(f'data/'+name+'.pkl', 'rb') as f:
            data = pickle.load(f)
        print(len(data))         #Sentences
        print(len(data[0]))      #type
        print(len(data[0][0]))   #The first sentence contains the number of words
        print("Raw data:", data[0])
                               
        #Data batching
        self.batch_data = self.sort_and_pad(data, batch_size)
        self.len_data = len(self.batch_data)

    def sort_and_pad(self, data, batch_size):
        #Calculate the total batch size 26546
        num_batch = int(math.ceil(len(data) / batch_size))
        #Sort by sentence length
        sorted_data = sorted(data, key=lambda x: len(x[0]))
        batch_data = list()
        
        #Get a batch of data
        for i in range(num_batch):
            batch_data.append(self.pad_data(sorted_data[i*int(batch_size) : (i+1)*int(batch_size)]))
        print("Batch output:", batch_data[100])
        
        return batch_data

    @staticmethod
    def pad_data(data_):
        #Define variable
        chars = []
        bounds = []
        flags = []
        targets = []
        
        #print("Sentences per batch:", len(data_))            #10
        #print("Each sentence contains the number of elements:", len(data_[0]))     #6
        #print("output data:", data_)
        
        max_length = max([len(sentence[0]) for sentence in data_])
        #print(max_length)
        
        #There are ten sets of data in each batch, and each set of data has four elements
        for line in data_:
            #char, bound, flag, target = line
            char, target, bound, flag = line
            padding = [0] * (max_length - len(char))    #Count the number of supplementary characters
            chars.append(char + padding)
            targets.append(target + padding)
            bounds.append(bound + padding)
            flags.append(flag + padding)

            
        return [chars, targets, bounds, flags]

    #Use one batch of data at a time
    def iter_batch(self, shuffle=False):
        if shuffle:
            random.shuffle(self.batch_data)
        for idx in range(self.len_data):
            yield self.batch_data[idx]
            
#-------------------------------Function: main function--------------------------------------
if __name__ == '__main__':
    #1.Splicing files
    #get_data_with_windows('train')

    #2. Batch processing
    train_data = BatchManager(10, 'train')
    
    #3. Process the test set data
    get_data_with_windows('test')

